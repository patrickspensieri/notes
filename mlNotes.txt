Decision Trees allow you to ask many linear questions, one after another

The role of the decision tree is to calculate the decision boundaries.

Entropy controls how a decision tree splits the data, by measuring impurity of examples.
Does so by recursively finding subsets that are as pure as possible.

Tradeoff between bias and variance

biased algorithms do not learn from data, while algorithms with high variance only perform well on data they've already seen.

Strengths and Weaknesses - Decision Trees ???

------------------------------------------------------------------------------------------------------------------

bagging is a type of ensemble technique, average out noisy, unbiased models to get a model with a lower variance

k nearest neighbors
	⁃	find labels nearest to the new point
	⁃	k = num neighbors to consider
	⁃	if num classes = 2, then k must be odd, to avoid ties
	⁃	k must not be a multiple of numm classes, to avoid ties
	⁃	complexity lies in finding nearest neighbors when datasets are large


adaboost
	⁃	bagging technique

random forest 
	⁃	bagging technique
	⁃	create many random decision trees, hence the term forest
	⁃	random trees are created by taking random subsets of test data for each tree
	⁃	errors in trees are possible, but rare
	⁃	if one tree only, no way to spot an error
	⁃	hence, with 5 trees, if 4/5 predict one class, and 1/5 predict another, likely that 1/5 is wrong

adaboost and random forest are ensemble methods

	⁃	get a general understanding
	⁃	head over to sklearn documentation
	⁃	get the algorithm running
	⁃	make predictions and evaluate

------------------------------------------------------------------------------------------------------------------

Regression - Continuous Supervised Learning

Discrete learning produces binary output, while Continous learning can deliver an entire range of values.

output = function(input)

r-squared score 
	⁃	max of 1, higher is better
	⁃	gages how well regression is doing
	⁃	used on test data
	⁃	using on training can also yield intro, good to compare

error = actual - predicted where the sign is important

sum of squared error
	⁃	goal is to minimize when performing linear regression
	⁃	gradient descent
	⁃	ordinary least squares (used in sklearn)
	⁃	creates a more balanced line (equidistant to most errors)
	⁃	makes computation of ideal line much easier
	⁃	not perfect because adding more points will almost always increase the sum of squared errors
	⁃	dependent on number of training points

When using sum of squared errors, adding more points will almost always increase the sum. So a line that has a better fit, but more points will be deemed worse than a line with a worse fit, if it has less data points.

r^2 
	⁃	max of 1, higher is better
	⁃	describes relationship between input and output in your line
	⁃	independent of number of training points
	⁃	gaging the r-squared value depends on the problem, some being harder than others

multivariate regression
	⁃	takes many features as inputs

outlier rejection
	⁃	train and test
	⁃	remove outliers with largest residuals (10% is ok)
	⁃	train and test again

Note that at times it is best to remove the outliers, if they are result of instrument error... However, outliers should be given extra attention in applications such as fraud detection

------------------------------------------------------------------------------------------------------------------

Unsupervised Learning is used to find structure in unlabeled data

K-Means Clustering
	⁃	assign (draw random cluster centers, and assign all points to the closest cluster center)
	⁃	optimize (minimize the total quadratic distance between a cluster center and its points... means that the cluster center will move closest to most points)
	⁃	repeat (by the end, the cluster center will move into the center of each cluster)

Issue: In K-Means, it is important that initial centroids are placed randomly. In situations where clulstering isn't obvious, initial points may dictate how the final points are split.

Solution: Repeat the clustering 10 times or more, average out

------------------------------------------------------------------------------------------------------------------

Feature Scaling involves preprocessing features, used in some machine learning algorithms

	⁃	Features may be imbalanced, take the height and the weight of a person
	⁃	Without normalizing, the weight will dominate the equation, since the number is much larger
	⁃	150lbs, and 5.9ft
	⁃	feature scaling will normalize all features, shape them as values between 0 and 1
	⁃	smallest value will get rescaled to 0
	⁃	largest will get rescaled to 1

normalized = (original - minVal) / float(maxVal - minVal)

	⁃	decision trees and linear regression don't need feature scaling
	⁃	svm and k-means clustering map features, use distance between points
	⁃	linear regression already has a individual coefficient associated with each feature

------------------------------------------------------------------------------------------------------------------

Text Learning

A Bag of Words is a dictionary mapping each word, to the number of times it occurs

from sklearn.feature_extraction.text import CountVectorizer

	⁃	stop words are words like a, the, of that are often removed before placing into a bag of words
	⁃	a bag of words should only take into account the stem of words, so loves, loving will be placed in the love bag
	⁃	word order doesn't matter (algorithm called stemmer)
	⁃	complex phrases, like Manchester United will be parsed as two separate words

Python NLTK (Natural Language Toolkit) can identify and remove stopwords.

Tfidf Representation (Term frequency Inverse document frequency)
	- term frequency is like a bag of words
	- inv document frequency places higher weights on rare words from a given corpus (email dataset)

------------------------------------------------------------------------------------------------------------------

Feature Selection

"Make everything as simple as possible, but no simpler" Albert Einstein

	- remove unecessary features, add new ones
	- if a feature has 100 percent correspondence with labels, then you dont need machine learning
	- remove features when they cause overfitting, they are too noisy or closely correlated to other features
	- in sklearn, SelectPercentile and SelectKBest can be used for feature selection
	- especially useful when working with rich data like text or images
	- using too few features will steer toward high bias

	high bias => doesn't pay attention to data (very high error on training)
	high variance => overfitting (very low error in training)

	regularization => balancing number of features and error

	- Lasso Regression minimizes SSE, like all regression except also tries to minimize num features
	- accomplishes this by taxing each additional feature being used (set coefficients of those features to 0)
	- can get the importance of each feature in scikit learn


------------------------------------------------------------------------------------------------------------------

PCA - Principal Component Analysis

	PCA is used for
	- transforming input features into Principal Components
	- compressing data while keeping trends by reducing dimensionality of data
	- PCs are directions with maximum variance in data
	- projecting/collapsing all other points onto the PC line minimizes information loss
	- PCs can be ranked by variance, more than one but they must all be perpandicular
	- max num PCs = number of input features (using all of them results in no compression)

	What does VARIANCE V(X) have to do with PCA?
	- goal is to rotate x and y axis, so that x axis is following general direction of data
	- calculated by finding line that has the highest spread, variance
	- choosing this line to place x-axis minimizes data loss
	- origin of graph placed over the center of mass
	- major axis dominates if the data looks like a line, less if oval
	- if circle, then variance is equal in all directions, lots of data loss

	When to use PCA?
	- dimensionality reduction of data, makes it easier to visualize
	- reduce noise (discarding weak PCs, keeping strongest trends)
	- preprocessing data to improve performance of other algorithms
	- Never perform feature selection before PCA, will lose data

	NOTE
	- When you fit the PCA, you must "transform" the data for actual compression to occur
	- When testing, don't fit PCA to testing features, but you must transform

------------------------------------------------------------------------------------------------------------------

Validation

	If training is complete using PCA and SVM
	- DO NOT fit PCA to test data (unfair advantage)
	- DO transform test data (dimensionality reduction to ensure form of data is the same)

	Question => How can data be transformed without being fit?

	K-fold Cross Validation
	- To increase accuracy, cross validation allows algorithm to both train and test on all data
	- Split into k bins, and train on k-1 bins, test on other
	- repeat the process k times and take the average	
	- when forming bins, it is important to randomize to eliminate any patterns

	Optimization of parameters
	- Cross validation can also be used to optimize parameters automatically, see GridSearchCV


